{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assessment of Cosmic Ray Direction Estimation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Required Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import json\n",
    "import os\n",
    "from glob import glob\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "from utils import get_cuts, get_event_parameters, get_training_assessment_cut, r_log"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model and Assessment Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The keys will be the names of the models you wish to analyze.\n",
    "# The values will be the nuclei to assess for each model.\n",
    "MODEL_NAMES_AND_NUCLEI = {\n",
    "    'direction_baseline': 'phof'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# IceTop-CNN folder in /data/user\n",
    "ICETOP_CNN_DATA_DIR = os.getenv('ICETOP_CNN_DATA_DIR')\n",
    "# Folder containing the models\n",
    "MODELS_FOLDER_PATH = os.path.join(ICETOP_CNN_DATA_DIR, 'models')\n",
    "# Folders containing the reconstructions\n",
    "X_RECONSTRUCTIONS_FOLDER_PATH = os.path.join(ICETOP_CNN_DATA_DIR, 'reconstructions', 'x_dir')\n",
    "Y_RECONSTRUCTIONS_FOLDER_PATH = os.path.join(ICETOP_CNN_DATA_DIR, 'reconstructions', 'y_dir')\n",
    "Z_RECONSTRUCTIONS_FOLDER_PATH = os.path.join(ICETOP_CNN_DATA_DIR, 'reconstructions', 'z_dir')\n",
    "# Folder containing the simulation data.\n",
    "SIMDATA_FOLDER_PATH = os.path.join(os.sep, 'data', 'user', 'fmcnally', 'icetop-cnn', 'simdata')\n",
    "# Various potential error messages.\n",
    "ERROR_MODELS_FOLDER_PATH_NOT_FOUND = 'Could not find models folder. Path specified: '\n",
    "ERROR_NO_MODELS_SELECTED = 'No models selected for analysis!'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### List Available Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure that the models folder has been found.\n",
    "assert os.path.exists(MODELS_FOLDER_PATH), f'{ERROR_MODELS_FOLDER_PATH_NOT_FOUND}{MODELS_FOLDER_PATH}'\n",
    "\n",
    "# Get all model reconstructions .npy files.\n",
    "model_list = glob(os.path.join(X_RECONSTRUCTIONS_FOLDER_PATH, '*.npy'))\n",
    "model_list = glob(os.path.join(Y_RECONSTRUCTIONS_FOLDER_PATH, '*.npy'))\n",
    "model_list = glob(os.path.join(Z_RECONSTRUCTIONS_FOLDER_PATH, '*.npy'))\n",
    "\n",
    "# Trim parent directories and file extension for each.\n",
    "model_list = [os.path.splitext(os.path.basename(model))[0] for model in model_list]\n",
    "# Get all model parameters .json files.\n",
    "param_list = glob(os.path.join(MODELS_FOLDER_PATH, '*', '*.json'))\n",
    "# Trim parent directories and file extension for each.\n",
    "param_list = [os.path.splitext(os.path.basename(param))[0] for param in param_list]\n",
    "\n",
    "# Print models that have both .npy and .json files.\n",
    "print(f'Available models: {sorted(set(model_list).intersection(param_list))}')\n",
    "# Print models that have a .npy file but no .json file.\n",
    "print(f'Models without parameter files: {sorted(set(model_list).difference(param_list))}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Model Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dictionary to store model parameters.\n",
    "model_parameters = {}\n",
    "# Loop over model names and their corresponding assessment nuclei.\n",
    "\n",
    "for model_name, assessment_nuclei in MODEL_NAMES_AND_NUCLEI.items():\n",
    "    \n",
    "    # Construct the full .json model path.\n",
    "    model_path = os.path.join(MODELS_FOLDER_PATH, model_name, model_name + '.json')\n",
    "\n",
    "    # Ensure that the model is found (no typos).\n",
    "    if not os.path.exists(model_path):\n",
    "        print(f'WARNING: Model {model_name} not found at {MODELS_FOLDER_PATH}')\n",
    "        continue\n",
    "\n",
    "    # Load model parameters and save into the dictionary.\n",
    "    with open(model_path, 'r') as f:\n",
    "        model_parameters[model_name] = json.load(f)\n",
    "    # Add assessment nuclei to model parameters. Sort the composition string into a predictable order.\n",
    "    model_parameters[model_name].update(\n",
    "        {'assessment_nuclei': ''.join(sorted(assessment_nuclei, key=lambda c: list('phof').index(c)))})\n",
    "    \n",
    "    # Print an entry for each model.\n",
    "    print(f'{model_name:>{max(map(len, MODEL_NAMES_AND_NUCLEI))}} : {model_parameters[model_name]}')\n",
    "\n",
    "# Ensure that at least one valid model has been selected for assessment.\n",
    "assert len(model_parameters), ERROR_NO_MODELS_SELECTED"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare Reconstructions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Event Parameters\n",
    "\n",
    "This might takes a few seconds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We must always load all nuclei. Each model will have a unique preference for which events to assess on.\n",
    "# Undesired events are cut next.\n",
    "event_parameters = get_event_parameters(SIMDATA_FOLDER_PATH, composition='phof')\n",
    "event_parameters['zenith'] = event_parameters['dir'][:, 0]\n",
    "event_parameters['azimuth'] = event_parameters['dir'][:, 1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Model Reconstructions and Generate Cuts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Map the nuclei string representations to their atomic weights.\n",
    "comp_conversion = {'p':1, 'h':4, 'o':16, 'f':56}\n",
    "\n",
    "# Build dictionaries for model reconstructions and events cuts.\n",
    "reconstructions, cuts = {}, {}\n",
    "# Loop over model names and their corresponding parameters.\n",
    "for model_name, model_prep in model_parameters.items():\n",
    "\n",
    "    # Load model predictions. It is important to flatten the predictions into a one-dimensional array.\n",
    "    x_reconstruction = 2 * np.load(os.path.join(X_RECONSTRUCTIONS_FOLDER_PATH, model_name + '.npy')).flatten() - 1\n",
    "    y_reconstruction = 2 * np.load(os.path.join(Y_RECONSTRUCTIONS_FOLDER_PATH, model_name + '.npy')).flatten() - 1\n",
    "    z_reconstruction = 2 * np.load(os.path.join(Z_RECONSTRUCTIONS_FOLDER_PATH, model_name + '.npy')).flatten() - 1\n",
    "    \n",
    "    # Normalize reconstructions\n",
    "    norm = np.sqrt(x_reconstruction**2 + y_reconstruction**2 + z_reconstruction**2)\n",
    "    x_reconstruction /= norm\n",
    "    y_reconstruction /= norm\n",
    "    z_reconstruction /= norm\n",
    "    \n",
    "    # Get the model-specific assessment cut.\n",
    "    model_cut = get_training_assessment_cut(event_parameters, 'assessment', model_prep)\n",
    "\n",
    "    # Get the list of atomic weights for the desired nuclei.\n",
    "    nuclei_to_assess = [comp_conversion[nuclei] for nuclei in model_prep['assessment_nuclei']]\n",
    "\n",
    "    # Apply the nuclei cut to the model predictions.\n",
    "    x_reconstruction = x_reconstruction[np.isin(event_parameters['comp'][model_cut], nuclei_to_assess)]\n",
    "    y_reconstruction = y_reconstruction[np.isin(event_parameters['comp'][model_cut], nuclei_to_assess)]\n",
    "    z_reconstruction = z_reconstruction[np.isin(event_parameters['comp'][model_cut], nuclei_to_assess)]\n",
    "\n",
    "    # Save the model predictions. For azimuth; if x is positive add pi/2. Otherwise, subtract pi/2.\n",
    "    zenith = np.arccos(z_reconstruction)\n",
    "    azimuth = np.arctan(y_reconstruction / x_reconstruction)\n",
    "    \n",
    "    azimuth[x_reconstruction < 0] += np.pi\n",
    "    azimuth[(x_reconstruction > 0) * (y_reconstruction < 0)] += 2 * np.pi\n",
    "    # Adjust the azimuth: if there is a positive x and negative y or a negative x and a negative y, add pi. If azimuth is less than 0, add 2 pi.\n",
    "    #azimuth[((x_reconstruction>0)*(y_reconstruction<0))+((x_reconstruction<0)*(y_reconstruction<0))] += np.pi\n",
    "    #azimuth[azimuth<0] += 2*np.pi\n",
    "\n",
    "    reconstructions[model_name] = np.array([zenith, azimuth])\n",
    "    # Get the model-specific events cut.\n",
    "    model_cut = np.isin(event_parameters['comp'], nuclei_to_assess) * model_cut\n",
    "    # Save the model-specific events cut.\n",
    "    cuts[model_name] = model_cut"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assess Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summary Statistics for Zenith"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# CONFIG FOR STATS\n",
    "log_difference, reco_percent = 0.03490658504, 10\n",
    "cut_names = ['Uncut', 'Quality']\n",
    "\n",
    "# Loop over each model's name and their corresponding reconstructed zeniths.\n",
    "for model_name, reconstructed_zenith in reconstructions.items():\n",
    "    # Print a header entry for each model.\n",
    "    print(f'Stats for {model_name}, zenith:')\n",
    "    # Loop over the cuts.\n",
    "    for cut_name in cut_names:\n",
    "        # Get cuts for the model reconstructions and event parameters.\n",
    "        reconstructions_cut, events_cut = get_cuts(cuts[model_name], event_parameters, cut_name)\n",
    "        # Calculate the event-wise difference in reconstructed vs true logged zeniths.\n",
    "        diff = reconstructed_zenith[0][reconstructions_cut] - event_parameters['zenith'][events_cut]\n",
    "        # Calculate the event-wise difference in reconstructed vs true unlogged zeniths.\n",
    "        true_diff = r_log(reconstructed_zenith[0][reconstructions_cut]) - r_log(event_parameters['zenith'][events_cut])\n",
    "        # Calculate the event-wise percent difference in reconstructed vs true unlogged zeniths.\n",
    "        percent_diff = 100 * true_diff / r_log(event_parameters['zenith'][events_cut])\n",
    "\n",
    "        # Calculate zenith resolution statistics for the logged zenith difference.\n",
    "        std_dev_lo, median, std_dev_hi = np.percentile(diff, (16,50,84))\n",
    "\n",
    "        # Calculate the percentage of events whose difference in true and reconstructed zeniths is less than \"log_difference\".\n",
    "        diff_percentage = 100 * len(list(filter(lambda x: abs(x) <= log_difference, diff))) / len(diff)\n",
    "        # Calculate the percentage of events whose reconstructed zeniths are within \"reco_percent\"% of their true zeniths.\n",
    "        percent_diff_percentage = 100 * len(list(filter(lambda x: abs(x) <= reco_percent, percent_diff))) / len(percent_diff)\n",
    "\n",
    "        # Print the summary statistics.\n",
    "        print(f'{\" \"*2}{cut_name}')\n",
    "        print(f'{\" \"*4}Zenith resolution: {median:.3f} +{std_dev_hi:.3f} {std_dev_lo:.3f}')\n",
    "        print(f'{\" \"*4}Events reconstructed within a {log_difference*180/np.pi:.2f} degree difference in order of magnitude: {diff_percentage:.2f}%')\n",
    "        print(f'{\" \"*4}Events reconstructed within {reco_percent}% of their true zeniths: {percent_diff_percentage:.2f}%')\n",
    "    \n",
    "    print() # Newline for readability\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summary Statistics for Azimuth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# CONFIG FOR STATS\n",
    "log_difference, reco_percent = 0.03490658504, 10\n",
    "cut_names = ['Uncut', 'Quality']\n",
    "\n",
    "# Loop over each model's name and their corresponding reconstructed azimuths.\n",
    "for model_name, reconstructed_azimuth in reconstructions.items():\n",
    "    # Print a header entry for each model.\n",
    "    print(f'Stats for {model_name}, azimuth:')\n",
    "    # Loop over the cuts.\n",
    "    for cut_name in cut_names:\n",
    "        # Get cuts for the model reconstructions and event parameters.\n",
    "        reconstructions_cut, events_cut = get_cuts(cuts[model_name], event_parameters, cut_name)\n",
    "        # Calculate the event-wise difference in reconstructed vs true logged azimuths. If the difference is above 180 degrees, subtract it from 360 degrees\n",
    "        diff = reconstructed_azimuth[1][reconstructions_cut] - event_parameters['azimuth'][events_cut]\n",
    "        # Calculate the event-wise difference in reconstructed vs true unlogged azimuths.\n",
    "        true_diff = r_log(reconstructed_azimuth[1][reconstructions_cut]) - r_log(event_parameters['azimuth'][events_cut])\n",
    "\n",
    "        #true_diff[np.abs(true_diff)>(10**(np.pi))] = np.abs(true_diff[np.abs(true_diff)>(10**(np.pi))]) - 10**(np.pi)\n",
    "\n",
    "        # Calculate the event-wise percent difference in reconstructed vs true unlogged azimuths.\n",
    "        percent_diff = 100 * true_diff / r_log(event_parameters['azimuth'][events_cut])\n",
    "\n",
    "        # Calculate azimuth resolution statistics for the logged azimuth difference.\n",
    "        std_dev_lo, median, std_dev_hi = np.percentile(diff, (16,50,84))\n",
    "\n",
    "        # Calculate the percentage of events whose difference in true and reconstructed azimuths is less than \"log_difference\".\n",
    "        diff_percentage = 100 * len(list(filter(lambda x: abs(x) <= log_difference, diff))) / len(diff)\n",
    "        # Calculate the percentage of events whose reconstructed azimuths are within \"reco_percent\"% of their true azimuths.\n",
    "        percent_diff_percentage = 100 * len(list(filter(lambda x: abs(x) <= reco_percent, percent_diff))) / len(percent_diff)\n",
    "\n",
    "        # Print the summary statistics.\n",
    "        print(f'{\" \"*2}{cut_name}')\n",
    "        print(f'{\" \"*4}Azimuth resolution: {median:.3f} +{std_dev_hi:.3f} {std_dev_lo:.3f}')\n",
    "        print(f'{\" \"*4}Events reconstructed within a {log_difference*180/np.pi:.2f} degree difference in order of magnitude: {diff_percentage:.2f}%')\n",
    "        print(f'{\" \"*4}Events reconstructed within {reco_percent}% of their true azimuths: {percent_diff_percentage:.2f}%')\n",
    "    \n",
    "    print() # Newline for readability"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logged Zenith Resolution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### CONFIG FOR PLOT\n",
    "cut_names = ['Uncut', 'Quality']\n",
    "hist_args = {'range':(-1, 1), 'bins':121, 'density':False, 'histtype':'step', 'log':True, 'linewidth':4}\n",
    "\n",
    "# CONFIG FOR TEXT\n",
    "title_params = {'fontsize':28}\n",
    "label_params = {'fontsize':20}\n",
    "legend_params = {'fontsize':14}\n",
    "\n",
    "# VARIABLES FOR PLOT\n",
    "ncols = len(cut_names)\n",
    "\n",
    "# Create the plot/subplots.\n",
    "fig, axs = plt.subplots(figsize=(13*ncols, 8), ncols=ncols)\n",
    "\n",
    "# If we are assessing only one cut, add an artificial dimension.\n",
    "if ncols == 1: axs = np.array([axs])\n",
    "# Loop over the cuts.\n",
    "for ax, cut_name in zip(axs, cut_names):\n",
    "    # Loop over the models.\n",
    "    for model_name in model_parameters:\n",
    "        \n",
    "        # Get cuts for the model reconstructions and event parameters.\n",
    "        reconstructions_cut, events_cut = get_cuts(cuts[model_name], event_parameters, cut_name)\n",
    "        \n",
    "        # Plot the logged difference histogram.\n",
    "        diff = reconstructions[model_name][0][reconstructions_cut] - event_parameters['zenith'][events_cut]\n",
    "        ax.hist((diff),\n",
    "                label=model_name, **hist_args)\n",
    "    # Plot a vertical line through the center of each plot.\n",
    "    # This line designates the ideal reconstruction.\n",
    "    ax.axvline()\n",
    "\n",
    "    # Decorate the plot.\n",
    "    ax.set_title(f'Zenith Resolution ({cut_name})', **title_params)\n",
    "    ax.set_xlabel(r'$θ_{\\mathrm{reco}} - θ_{\\mathrm{true}}$', **label_params)\n",
    "    ax.set_ylabel('$\\log_{10}(Counts)$', **label_params)\n",
    "    ax.legend(**legend_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logged Azimuth Resolution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CONFIG FOR PLOT\n",
    "cut_names = ['Uncut', 'Quality']\n",
    "hist_args = {'range':(-np.pi,np.pi), 'bins':121, 'density':False, 'histtype':'step', 'log':True, 'linewidth':4}\n",
    "\n",
    "# CONFIG FOR TEXT\n",
    "title_params = {'fontsize':28}\n",
    "label_params = {'fontsize':20}\n",
    "legend_params = {'fontsize':14}\n",
    "\n",
    "# VARIABLES FOR PLOT\n",
    "ncols = len(cut_names)\n",
    "\n",
    "# Create the plot/subplots.\n",
    "fig, axs = plt.subplots(figsize=(13*ncols, 8), ncols=ncols)\n",
    "\n",
    "# If we are assessing only one cut, add an artificial dimension.\n",
    "if ncols == 1: axs = np.array([axs])\n",
    "# Loop over the cuts.\n",
    "for ax, cut_name in zip(axs, cut_names):\n",
    "    # Loop over the models.\n",
    "    for model_name in model_parameters:\n",
    "        \n",
    "        # Get cuts for the model reconstructions and event parameters.\n",
    "        reconstructions_cut, events_cut = get_cuts(cuts[model_name], event_parameters, cut_name)\n",
    "\n",
    "        # Plot the logged difference histogram.\n",
    "        diff = reconstructions[model_name][1][reconstructions_cut] - event_parameters['azimuth'][events_cut]\n",
    "        \n",
    "        ax.hist((diff),\n",
    "                label=model_name, **hist_args)\n",
    "    # Plot a vertical line through the center of each plot.\n",
    "    # This line designates the ideal reconstruction.\n",
    "    ax.axvline()\n",
    "\n",
    "    # Decorate the plot.\n",
    "    ax.set_title(f'Azimuth Resolution ({cut_name})', **title_params)\n",
    "    ax.set_xlabel(r'$\\phi_{\\mathrm{reco}} - \\phi_{\\mathrm{true}}$', **label_params)\n",
    "    ax.set_ylabel('$\\log_{10}(Counts)$', **label_params)\n",
    "    ax.legend(**legend_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Unlogged Zenith Resolution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CONFIG FOR PLOT\n",
    "cut_names = ['Uncut', 'Quality']\n",
    "hist_args = {'range':(-10,10), 'bins':121, 'density':False, 'histtype':'step', 'log':False, 'linewidth':4}\n",
    "\n",
    "# CONFIG FOR TEXT\n",
    "title_params = {'fontsize':28}\n",
    "label_params = {'fontsize':20}\n",
    "legend_params = {'fontsize':14}\n",
    "\n",
    "# VARIABLES FOR PLOT\n",
    "ncols = len(cut_names)\n",
    "\n",
    "# Create the plot/subplots.\n",
    "fig, axs = plt.subplots(figsize=(13*ncols, 8), ncols=ncols)\n",
    "\n",
    "# If we are assessing only one cut, add an artificial dimension.\n",
    "if ncols == 1: axs = np.array([axs])\n",
    "# Loop over the cuts.\n",
    "for ax, cut_name in zip(axs, cut_names):\n",
    "    # Loop over the models.\n",
    "    for model_name in model_parameters:\n",
    "\n",
    "        # Get cuts for the model reconstructions and event parameters.\n",
    "        reconstructions_cut, events_cut = get_cuts(cuts[model_name], event_parameters, cut_name)\n",
    "\n",
    "        # Plot the zoomed and unlogged difference histogram.\n",
    "        ax.hist((180/np.pi*reconstructions[model_name][0][reconstructions_cut] - 180/np.pi*event_parameters['zenith'][events_cut]),\n",
    "                label=model_name, **hist_args)\n",
    "\n",
    "    # Plot a vertical line through the center of each plot.\n",
    "    # This line designates the ideal reconstruction.\n",
    "    ax.axvline()\n",
    "    \n",
    "    # Decorate the plot.\n",
    "    ax.set_title(f'Zenith Resolution ({cut_name})', **title_params)\n",
    "    ax.set_xlabel(r'$θ\\degree_{\\mathrm{reco}} - θ\\degree_{\\mathrm{true}}$', **label_params)\n",
    "    ax.set_ylabel('Counts', **label_params)\n",
    "    ax.legend(**legend_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Unlogged Azimuth Resolution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CONFIG FOR PLOT\n",
    "cut_names = ['Uncut', 'Quality']\n",
    "hist_args = {'range':(-10,10), 'bins':121, 'density':False, 'histtype':'step', 'log':False, 'linewidth':4}\n",
    "\n",
    "# CONFIG FOR TEXT\n",
    "title_params = {'fontsize':28}\n",
    "label_params = {'fontsize':20}\n",
    "legend_params = {'fontsize':14}\n",
    "\n",
    "# VARIABLES FOR PLOT\n",
    "ncols = len(cut_names)\n",
    "\n",
    "# Create the plot/subplots.\n",
    "fig, axs = plt.subplots(figsize=(13*ncols, 8), ncols=ncols)\n",
    "\n",
    "# If we are assessing only one cut, add an artificial dimension.\n",
    "if ncols == 1: axs = np.array([axs])\n",
    "# Loop over the cuts.\n",
    "for ax, cut_name in zip(axs, cut_names):\n",
    "    # Loop over the models.\n",
    "    for model_name in model_parameters:\n",
    "\n",
    "        # Get cuts for the model reconstructions and event parameters.\n",
    "        reconstructions_cut, events_cut = get_cuts(cuts[model_name], event_parameters, cut_name)\n",
    "\n",
    "        # Plot the zoomed and unlogged difference histogram.\n",
    "        ax.hist((180/np.pi*reconstructions[model_name][1][reconstructions_cut] - 180/np.pi*event_parameters['azimuth'][events_cut]),\n",
    "                label=model_name, **hist_args)\n",
    "\n",
    "    # Plot a vertical line through the center of each plot.\n",
    "    # This line designates the ideal reconstruction.\n",
    "    ax.axvline()\n",
    "    \n",
    "    # Decorate the plot.\n",
    "    ax.set_title(f'Azimuth Resolution ({cut_name})', **title_params)\n",
    "    ax.set_xlabel(r'$\\phi\\degree_{\\mathrm{reco}} - \\phi\\degree_{\\mathrm{true}}$', **label_params)\n",
    "    ax.set_ylabel('Counts', **label_params)\n",
    "    ax.legend(**legend_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Opening-angle Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### CONFIG FOR PLOT\n",
    "cut_names = ['Uncut', 'Quality']\n",
    "hist_args = {'range':(0, 180), 'bins':121, 'density':True, 'histtype':'step', 'log':True, 'linewidth':4}\n",
    "\n",
    "# CONFIG FOR TEXT\n",
    "title_params = {'fontsize':28}\n",
    "label_params = {'fontsize':20}\n",
    "legend_params = {'fontsize':14}\n",
    "\n",
    "# VARIABLES FOR PLOT\n",
    "ncols = len(cut_names)\n",
    "\n",
    "# Create the plot/subplots.\n",
    "fig, axs = plt.subplots(figsize=(13*ncols, 8), ncols=ncols)\n",
    "\n",
    "# If we are assessing only one cut, add an artificial dimension.\n",
    "if ncols == 1: axs = np.array([axs])\n",
    "# Loop over the cuts.\n",
    "for ax, cut_name in zip(axs, cut_names):\n",
    "    # Loop over the models.\n",
    "    for model_name in model_parameters:\n",
    "\n",
    "        # Get cuts for the model reconstructions and event parameters.\n",
    "        reconstructions_cut, events_cut = get_cuts(cuts[model_name], event_parameters, cut_name)\n",
    "        # Plot the zoomed and unlogged difference histogram.\n",
    "        true_zenith = 0.5 * np.pi - event_parameters['zenith'][events_cut]\n",
    "        reco_zenith = 0.5 * np.pi - reconstructions[model_name][0][reconstructions_cut]\n",
    "        \n",
    "        delta_azimuth = reconstructions[model_name][1][reconstructions_cut] - event_parameters['azimuth'][events_cut]\n",
    "        delta_omega = 180 / np.pi * np.arccos(np.sin(reco_zenith) * np.sin(true_zenith) + np.cos(reco_zenith) * np.cos(true_zenith) * np.cos(delta_azimuth))\n",
    "        print(f'68% ({cut_name}): {np.percentile(delta_omega, 68)}')\n",
    "        print(f'Mean ({cut_name}): {np.mean(delta_omega)}')\n",
    "        ms = 0\n",
    "        for i in delta_omega:\n",
    "            ms += i\n",
    "        ms /= len(delta_omega)\n",
    "        rms = np.sqrt(ms)\n",
    "        print(f'RMS ({cut_name}): {rms}')\n",
    "        \n",
    "        ax.hist((delta_omega),\n",
    "        label=model_name, **hist_args)\n",
    "\n",
    "    # Plot a vertical line through the center of each plot.\n",
    "    # This line designates the ideal reconstruction.\n",
    "    ax.axvline(0)\n",
    "    \n",
    "    # Decorate the plot.\n",
    "    ax.set_title(f'Direction Resolution ({cut_name})', **title_params)\n",
    "    ax.set_xlabel(r'Opening Angle', **label_params)\n",
    "    ax.set_ylabel('Counts', **label_params)\n",
    "    ax.legend(**legend_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "### Two-Dimensional Visualization of Zenith"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CONFIG FOR PLOT\n",
    "cut_names = ['Uncut', 'Quality']\n",
    "zenith_range = (2, np.pi)\n",
    "nbins = 180\n",
    "\n",
    "# CONFIG FOR TEXT\n",
    "title_params = {'fontsize':28}\n",
    "label_params = {'fontsize':20}\n",
    "tick_params = {'axis':'both', 'direction':'out', 'labelsize':14}\n",
    "\n",
    "# VARIABLES FOR PLOT\n",
    "ncols, nrows = len(cut_names), len(model_parameters)\n",
    "bin_edges = np.linspace(*zenith_range, nbins+1)\n",
    "bin_centers = (bin_edges[:-1] + bin_edges[1:]) / 2\n",
    "\n",
    "# Create the plot/subplots.\n",
    "fig, axs = plt.subplots(figsize=(13*ncols, 10*nrows), ncols=ncols, nrows=nrows, sharex=True, sharey=True)\n",
    "\n",
    "# If we are assessing only one model, add an artificial dimension.\n",
    "if nrows == 1: axs = np.array([axs])\n",
    "# Loop over the models.\n",
    "for model_ax, model_name in zip(axs, model_parameters):\n",
    "    # If we are assessing only one cut, add an artificial dimension.\n",
    "    if ncols == 1: model_ax = np.array([model_ax])\n",
    "    # Loop over the cuts.\n",
    "    for ax, cut_name in zip(model_ax, cut_names):\n",
    "        \n",
    "        # Get cuts for the model reconstructions and event parameters.\n",
    "        reconstructions_cut, events_cut = get_cuts(cuts[model_name], event_parameters, cut_name)\n",
    "\n",
    "        # Compute the 2D histogram.\n",
    "        hist, _, _ = np.histogram2d(reconstructions[model_name][0][reconstructions_cut], event_parameters['zenith'][events_cut],\n",
    "                                    bins=(bin_edges, bin_edges))\n",
    "\n",
    "        # Normalize the histogram along each true zenith bin.\n",
    "        # This special way of dividing avoids any divide-by-zero errors.\n",
    "        hist = np.divide(hist, np.sum(hist, axis=0), out=np.zeros_like(hist), where=np.sum(hist, axis=0) != 0)\n",
    "        \n",
    "        # Plot the logged 2D histogram.\n",
    "        # Ignore divide-by-zero errors caused by a value of 0 in any bin.\n",
    "        with np.errstate(divide='ignore'):\n",
    "            im = ax.imshow(np.log10(hist), extent=(*zenith_range, *zenith_range), vmin=-3.5, vmax=-0.5, origin='lower')\n",
    "\n",
    "        # Plot a diagonal line through each plot.\n",
    "        # This line designates the ideal reconstruction.\n",
    "        ax.plot(zenith_range, zenith_range, color='black', linestyle=':')\n",
    "\n",
    "        # Create contour lines for one and two standard deviations on either side of the median.\n",
    "        # These values are determined for each true energy bin. They definitely need to be determined for each zenith bin.\n",
    "        contour_values = [.025, .16, .84, .975]\n",
    "        # Take the cumulative sum of the array, a range [0-1]. We can then look at where we would insert\n",
    "        #   each contour value to determine the heights for the stairs. For columns with no entries, the\n",
    "        #   resulting list indices will be past the end of the list. We can use the indices modulo the number\n",
    "        #   of bins to set the problematic list indices to 0.\n",
    "        contour_indices = np.asarray([np.searchsorted(col, contour_values) for col in np.cumsum(hist, axis=0).transpose()]) % nbins\n",
    "        for contour in contour_indices.transpose():\n",
    "            ax.stairs(bin_centers[contour], edges=bin_edges, color='red', linestyle='--')\n",
    "        \n",
    "        # Create a colorbar.\n",
    "        cbar = fig.colorbar(im, ax=ax)\n",
    "        cbar.ax.tick_params(**tick_params)\n",
    "        \n",
    "        # Set the y-axis limits to be the zenith range.\n",
    "        ax.set_ylim(zenith_range)\n",
    "\n",
    "        # Decorate the plot.\n",
    "        ax.set_title(f'{model_name} ({cut_name})', **title_params)\n",
    "        ax.set_xlabel(r'$\\theta\\degree_{\\mathrm{true}}$', **label_params)\n",
    "        ax.set_ylabel(r'$\\theta\\degree_{\\mathrm{reco}}$', **label_params)\n",
    "        ax.tick_params(**tick_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Two-Dimensional Visualization of Azimuth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CONFIG FOR PLOT\n",
    "cut_names = ['Uncut', 'Quality']\n",
    "azimuth_range = (0, 2 * np.pi)\n",
    "nbins = 180\n",
    "\n",
    "# CONFIG FOR TEXT\n",
    "title_params = {'fontsize':28}\n",
    "label_params = {'fontsize':20}\n",
    "tick_params = {'axis':'both', 'direction':'out', 'labelsize':14}\n",
    "\n",
    "# VARIABLES FOR PLOT\n",
    "ncols, nrows = len(cut_names), len(model_parameters)\n",
    "bin_edges = np.linspace(*azimuth_range, nbins+1)\n",
    "bin_centers = (bin_edges[:-1] + bin_edges[1:]) / 2\n",
    "\n",
    "# Create the plot/subplots.\n",
    "fig, axs = plt.subplots(figsize=(13*ncols, 10*nrows), ncols=ncols, nrows=nrows, sharex=True, sharey=True)\n",
    "\n",
    "# If we are assessing only one model, add an artificial dimension.\n",
    "if nrows == 1: axs = np.array([axs])\n",
    "# Loop over the models.\n",
    "for model_ax, model_name in zip(axs, model_parameters):\n",
    "    # If we are assessing only one cut, add an artificial dimension.\n",
    "    if ncols == 1: model_ax = np.array([model_ax])\n",
    "    # Loop over the cuts.\n",
    "    for ax, cut_name in zip(model_ax, cut_names):\n",
    "        \n",
    "        # Get cuts for the model reconstructions and event parameters.\n",
    "        reconstructions_cut, events_cut = get_cuts(cuts[model_name], event_parameters, cut_name)\n",
    "\n",
    "        # Compute the 2D histogram.\n",
    "        hist, _, _ = np.histogram2d(reconstructions[model_name][1][reconstructions_cut], event_parameters['azimuth'][events_cut],\n",
    "                                    bins=(bin_edges, bin_edges))\n",
    "\n",
    "        # Normalize the histogram along each true azimuth bin.\n",
    "        # This special way of dividing avoids any divide-by-zero errors.\n",
    "        hist = np.divide(hist, np.sum(hist, axis=0), out=np.zeros_like(hist), where=np.sum(hist, axis=0) != 0)\n",
    "        \n",
    "        # Plot the logged 2D histogram.\n",
    "        # Ignore divide-by-zero errors caused by a value of 0 in any bin.\n",
    "        with np.errstate(divide='ignore'):\n",
    "            im = ax.imshow(np.log10(hist), extent=(*azimuth_range, *azimuth_range), vmin=-3.5, vmax=-0.5, origin='lower')\n",
    "\n",
    "        # Plot a diagonal line through each plot.\n",
    "        # This line designates the ideal reconstruction.\n",
    "        ax.plot(azimuth_range, azimuth_range, color='black', linestyle=':')\n",
    "\n",
    "        # Create contour lines for one and two standard deviations on either side of the median.\n",
    "        # These values are determined for each true energy bin. They definitely need to be determined for each azimuth bin.\n",
    "        contour_values = [.025, .16, .84, .975]\n",
    "        # Take the cumulative sum of the array, a range [0-1]. We can then look at where we would insert\n",
    "        #   each contour value to determine the heights for the stairs. For columns with no entries, the\n",
    "        #   resulting list indices will be past the end of the list. We can use the indices modulo the number\n",
    "        #   of bins to set the problematic list indices to 0.\n",
    "        contour_indices = np.asarray([np.searchsorted(col, contour_values) for col in np.cumsum(hist, axis=0).transpose()]) % nbins\n",
    "        for contour in contour_indices.transpose():\n",
    "            ax.stairs(bin_centers[contour], edges=bin_edges, color='red', linestyle='--')\n",
    "        \n",
    "        # Create a colorbar.\n",
    "        cbar = fig.colorbar(im, ax=ax)\n",
    "        cbar.ax.tick_params(**tick_params)\n",
    "        \n",
    "        # Set the y-axis limits to be the azimuth range.\n",
    "        ax.set_ylim(azimuth_range)\n",
    "\n",
    "        # Decorate the plot.\n",
    "        ax.set_title(f'{model_name} ({cut_name})', **title_params)\n",
    "        ax.set_xlabel(r'$\\phi\\degree_{\\mathrm{true}}$', **label_params)\n",
    "        ax.set_ylabel(r'$\\phi\\degree_{\\mathrm{reco}}$', **label_params)\n",
    "        ax.tick_params(**tick_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Zenith Resolution as a Function of Energy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CONFIG FOR PLOT\n",
    "cut_names = ['Uncut']\n",
    "e_range = (5, 8)\n",
    "nbins = 49\n",
    "comparison_reconstruction = 'laputop'\n",
    "errorbar_args = {'fmt':'.', 'markersize':40, 'elinewidth':2, 'capsize':10, 'capthick':2}\n",
    "\n",
    "# CONFIG FOR TEXT\n",
    "title_params = {'fontsize':28}\n",
    "label_params = {'fontsize':20}\n",
    "tick_params = {'labelsize':14}\n",
    "legend_params = {'fontsize':14, 'markerscale':0.5}\n",
    "\n",
    "# VARIABLES FOR PLOT\n",
    "ncols = len(cut_names)\n",
    "bin_edges = np.linspace(*e_range, nbins+1)\n",
    "bin_centers = (bin_edges[:-1] + bin_edges[1:]) / 2\n",
    "\n",
    "# Create the plot/subplots.\n",
    "fig, axs = plt.subplots(figsize=(26, 10), ncols=ncols)\n",
    "\n",
    "# If we are assessing only one cut, add an artificial dimension.\n",
    "if ncols == 1: axs = np.array([axs])\n",
    "# Loop over the cuts.\n",
    "for ax, cut_name in zip(axs, cut_names):\n",
    "    # Loop over the models.\n",
    "    for model_name in model_parameters:\n",
    "        \n",
    "        # Get cuts for the model reconstructions and event parameters.\n",
    "        reconstructions_cut, events_cut = get_cuts(cuts[model_name], event_parameters, cut_name)\n",
    "        # Bin each event by its energy.\n",
    "        binned_energy = np.digitize(event_parameters['energy'][events_cut], bin_edges)\n",
    "        # Create an empty array to hold the statistics for each bin.\n",
    "        binned_statistics = np.zeros((nbins, 3))\n",
    "        # Loop over the bins.\n",
    "        for bin in range(nbins):\n",
    "            # Create a bin-specific cut. This is how we will generate statistics for each bin.\n",
    "            bin_cut = (binned_energy == bin)\n",
    "            # No data for a particular bin means we can skip it.\n",
    "            if not np.any(bin_cut): continue\n",
    "            # Generate the statistics for each bin.\n",
    "            binned_statistics[bin] = np.percentile(\n",
    "                reconstructions[model_name][0][reconstructions_cut][bin_cut] - event_parameters['zenith'][events_cut][bin_cut],\n",
    "                (50, 16, 84))\n",
    "        # Extract each statistic separately.\n",
    "        median, err_min, err_max = np.transpose(binned_statistics)\n",
    "        # Plot the zenith resolution as a function of energy.\n",
    "        ax.errorbar(bin_centers, median, yerr=(median-err_min, err_max-median), label=model_name, **errorbar_args)\n",
    "    # Plot a horizontal line through the center of each plot.\n",
    "    # This line designates the ideal reconstruction.\n",
    "    ax.axhline(color='black', ls='--')\n",
    "    \n",
    "    # Set the y-axis limits to be within a reasonable range.\n",
    "    ax.set_ylim(-0.2, 0.2)\n",
    "    \n",
    "    # Decorate the plot.\n",
    "    ax.set_title(f'Zenith Resolution v. Energy ({cut_name})', **title_params)\n",
    "    ax.set_xlabel(r'$\\log_{10}(E_{true}/GeV)$', loc='right', **label_params)\n",
    "    ax.set_ylabel(r'$\\theta_{\\mathrm{reco}} - \\theta_{\\mathrm{true}}$', **label_params)\n",
    "    ax.tick_params(**tick_params)\n",
    "    ax.legend(**legend_params)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Azimuth Resolution as a Function of Energy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CONFIG FOR PLOT\n",
    "cut_names = ['Uncut']\n",
    "e_range = (5, 8)\n",
    "nbins = 49\n",
    "comparison_reconstruction = 'laputop'\n",
    "errorbar_args = {'fmt':'.', 'markersize':40, 'elinewidth':2, 'capsize':10, 'capthick':2}\n",
    "\n",
    "# CONFIG FOR TEXT\n",
    "title_params = {'fontsize':28}\n",
    "label_params = {'fontsize':20}\n",
    "tick_params = {'labelsize':14}\n",
    "legend_params = {'fontsize':14, 'markerscale':0.5}\n",
    "\n",
    "# VARIABLES FOR PLOT\n",
    "ncols = len(cut_names)\n",
    "bin_edges = np.linspace(*e_range, nbins+1)\n",
    "bin_centers = (bin_edges[:-1] + bin_edges[1:]) / 2\n",
    "\n",
    "# Create the plot/subplots.\n",
    "fig, axs = plt.subplots(figsize=(26, 10), ncols=ncols)\n",
    "\n",
    "# If we are assessing only one cut, add an artificial dimension.\n",
    "if ncols == 1: axs = np.array([axs])\n",
    "# Loop over the cuts.\n",
    "for ax, cut_name in zip(axs, cut_names):\n",
    "    # Loop over the models.\n",
    "    for model_name in model_parameters:\n",
    "        \n",
    "        # Get cuts for the model reconstructions and event parameters.\n",
    "        reconstructions_cut, events_cut = get_cuts(cuts[model_name], event_parameters, cut_name)\n",
    "        # Bin each event by its energy.\n",
    "        binned_energy = np.digitize(event_parameters['energy'][events_cut], bin_edges)\n",
    "        # Create an empty array to hold the statistics for each bin.\n",
    "        binned_statistics = np.zeros((nbins, 3))\n",
    "        # Loop over the bins.\n",
    "        for bin in range(nbins):\n",
    "            # Create a bin-specific cut. This is how we will generate statistics for each bin.\n",
    "            bin_cut = (binned_energy == bin)\n",
    "            # No data for a particular bin means we can skip it.\n",
    "            if not np.any(bin_cut): continue\n",
    "            # Generate the statistics for each bin.\n",
    "            binned_statistics[bin] = np.percentile(\n",
    "                reconstructions[model_name][1][reconstructions_cut][bin_cut] - event_parameters['azimuth'][events_cut][bin_cut],\n",
    "                (50, 16, 84))\n",
    "        # Extract each statistic separately.\n",
    "        median, err_min, err_max = np.transpose(binned_statistics)\n",
    "        # Plot the azimuth resolution as a function of energy.\n",
    "        ax.errorbar(bin_centers, median, yerr=(median-err_min, err_max-median), label=model_name, **errorbar_args)\n",
    "    # Plot a horizontal line through the center of each plot.\n",
    "    # This line designates the ideal reconstruction.\n",
    "    ax.axhline(color='black', ls='--')\n",
    "    \n",
    "    # Set the y-axis limits to be within a reasonable range.\n",
    "    ax.set_ylim(-0.5 * np.pi, 0.5 * np.pi)\n",
    "    \n",
    "    # Decorate the plot.\n",
    "    ax.set_title(f'Azimuth Resolution v. Energy ({cut_name})', **title_params)\n",
    "    ax.set_xlabel(r'$\\log_{10}(E_{true}/GeV)$', loc='right', **label_params)\n",
    "    ax.set_ylabel(r'$\\phi_{\\mathrm{reco}} - \\phi_{\\mathrm{true}}$', **label_params)\n",
    "    ax.tick_params(**tick_params)\n",
    "    ax.legend(**legend_params)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "IceTop-CNN",
   "language": "python",
   "name": "icetop-cnn"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
